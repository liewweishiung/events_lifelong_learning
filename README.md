## Lifelong Learning from Event-based Data
### The lifelong learning module.
2021 Vadym Gryshchuk (vadym.gryshchuk@protonmail.com)

See the License

We study lifelong learning from data generated by event cameras. Contemporary methods for incremental learning are predominantly based on the frame-based data recorded by conventional shutter cameras. An event camera delivers high dynamic range, low power consumption, and high temporal resolution, thus making it suitable for dynamic environments, in which knowledge must be accumulated incrementally. We propose an architecture for lifelong learning that is composed of two modules: a feature extractor and an incremental learner. The feature extractor is utilized as a self-supervised sparse convolutional neural network, which processes event-based data. The incremental learner uses a habituation-based method that works in tandem with other existing techniques to mitigate catastrophic forgetting. The conducted experiments show that our proposed method is capable of effective incremental learning without forgetting previously learned experiences.

The following figure illustrates the architecture.

![Architecture](./architecture.svg)


This is the implementation for the upper module of the architecture for Lifelong Learning. 

### Installation 

1. `conda env create --file env.yml`


### Data Collection
(The extracted features of `NCALTECH12` for testing purposes are already provided.)

1. See the code for the feature extraction module: https://github.com/VadymV/events_feature_extractor
2. Place the extracted features into the `store` folder. The following tree should be created: `store` -> `store` -> `datasets` -> [`ncaltech12`, `ncaltech256`] -> [`training`, `testing`]


### Short learning horizon. Scenario 1 

1. In`settings.yaml` set the dataset `name` to `NCALTECH12`, the habituation `decay_rate` to `0.2`, the habituation `top_neurons` to `0.02`, `scenario` to `1`, `iterations` to `100`, `batch_size` to `64`, `z_dimension` to `100`, `batch_normalization` to `false`, `tasks` to `6`, `number_seeds` to `10`
2. Run `python run_ncaltech.py`

### Short learning horizon. Scenario 2

1. In`settings.yaml` set the dataset `name` to `NCALTECH12`, the habituation `decay_rate` to `0.001`, the habituation `top_neurons` to `0.02`, `scenario` to `2`, `iterations` to `100`, `batch_size` to `64`, `z_dimension` to `100`, `batch_normalization` to `false`, `tasks` to `6`, , `number_seeds` to `10`
2. Run `python run_ncaltech.py`


### Long learning horizon. 16 episodes. Scenario 2

1. In`settings.yaml` set the dataset `name` to `NCALTECH256`, the habituation `decay_rate` to `0.001`, the habituation `top_neurons` to `0.01`, `scenario` to `2`, `iterations` to `500`, `batch_size` to `256`, `z_dimension` to `257`, `batch_normalization` to `true`, `tasks` to `16`, `number_seeds` to `3`
2. Run `python run_ncaltech.py`

### Long learning horizon. 32 episodes. Scenario 2

1. In`settings.yaml` set the dataset `name` to `NCALTECH256`, the habituation `decay_rate` to `0.001`, the habituation `top_neurons` to `0.01`, `scenario` to `2`, `iterations` to `320`, `batch_size` to `256`, `z_dimension` to `257`, `batch_normalization` to `true`, `tasks` to `32`, `number_seeds` to `3`
2. Run `python run_ncaltech.py`

---
**The provided code uses the code from https://github.com/GMvandeVen/brain-inspired-replay. See the header of each file for more information**
